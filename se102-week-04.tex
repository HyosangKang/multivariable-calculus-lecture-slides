\documentclass{beamer}
\usefonttheme{serif}

\usepackage{amsmath,amsthm,amssymb,amsfonts,amscd,mathrsfs,amsxtra,multirow,kotex,mathtools,gensymb,textcomp,lipsum,tikz,verbatim,color,soul,courier,mdframed,xcolor}
\usepackage[normalem]{ulem}
\usetikzlibrary{calc,matrix,arrows,chains,positioning,scopes}
\usepackage{pdfpages}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{excs}[thm]{Exercise}
\newtheorem{rem}[thm]{Remark}
\newtheorem{prob}[thm]{Problem}
\newtheorem{cor}[thm]{Corollary}

\newcommand \tr[1]{\textcolor{red}{#1}}
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\newcommand{\varep}{\varepsilon}
\newcommand{\DrawBox}[1][]{%
    \tikz[overlay,remember picture]{
    \draw[red,#1]
      ($(left)+(-0.2em,0.9em)$) rectangle
      ($(right)+(0.2em,-0.3em)$);}
}

\newcommand{\tikzmarkk}[2]{
    \tikz[overlay,remember picture,baseline] 
    \node[anchor=base] (#1) {$#2$};
}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\tikzset{join/.code=\tikzset{after node path={%
\ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
edge[every join]#1(\tikzchaincurrent)\fi}}}

\tikzset{>=stealth',every on chain/.append style={join},
         every join/.style={->}}
\tikzstyle{labeled}=[execute at begin node=$\scriptstyle,
   execute at end node=$]

\newenvironment<>{proofs}[1][\proofname]{%
   \par
   \def\insertproofname{#1\@{.}}%
   \usebeamertemplate{proof begin}#2}
 {\usebeamertemplate{proof end}}
 

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \raisebox{2pt}[0pt][0pt]{\insertframenumber/\inserttotalframenumber}
}
\setbeamercolor{footline}{fg=blue}
\setbeamerfont{footline}{series=\bfseries}
\title[]{SE102:Multivariable Calculus}

\author[]{Hyosang Kang\inst{1}}

\institute[]{\inst{1}Division of Mathematics\\ School of Interdisciplinary Studies\\ DGIST}

\date[]{Week 04}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\begin{thm}[Clairaut]
If the partial derivatives $f_{xy}$, $f_{yx}$ 
are continuous at $(x_0,y_0)$, then
	$$f_{xy}(x_0,y_0)=f_{yx}(x_0,y_0)$$
\end{thm}
\begin{defn}
Suppose that $f(x,y)$ has continuous second partial derivatives 
at $(x_0,y_0)$. Then the polynomial 
	\begin{align*}
	Q(&x,y)= f(x_0,y_0)+f_x(x_0,y_0)(x-x_0)\\
	&+f_y(x_0,y_0)(y-y_0)+\frac{1}{2}f_{xx}(x_0,y_0)(x-x_0)^2\\
	&+f_{xy}(x_0,y_0)(x-x_0)(y-y_0)\\
	&+\frac{1}{2}f_{yy}(x_0,y_0)(y-y_0)^2
	\end{align*}
is called the \textbf{Taylor polynomial of second degree}
of $f$ at $(x_0,y_0)$.
\end{defn}
\end{frame}

\begin{frame}
\begin{rem}
If all $n$-th order partial derivatives of a function $f(x,y)$
are continuous, then
\begin{itemize}
    \item $f$ is differentiable, and
    \item the formulae of $n$-order partial derivatives
    does not depend on the order of partial derivatives.
\end{itemize}
\end{rem}
\begin{rem}
Let 
	$$\Delta\mathbf x=(\Delta x,\Delta y)=(x-x_0,y-y_0)$$ 
and denote the \emph{gradient operator} $\nabla$ 
in vector notation:
    $$\nabla=\left(\frac{\partial }{\partial x},
    \frac{\partial }{\partial y}\right)$$
\end{rem}
\end{frame}

\begin{frame}
Let us define the \emph{multiplication} 
of differential operators as follows:
    $$\left(\frac{\partial}{\partial x}
     \frac{\partial}{\partial x}\right)f
     = \frac{\partial^2f}{\partial x^2},\quad
    \left(\frac{\partial}{\partial x}
     \frac{\partial}{\partial y}\right)f
     = \frac{\partial^2f}{\partial x\partial y}$$
By the assumption in the definition of Taylor polynomial,
we have $f_{xy}(x_0,y_0)=f_{yx}(x_0,y_0)$.
Thus we can write
    $$Q(x,y)=\sum_{n=0}^2\frac{1}{n!}
    (\Delta\mathbf x\cdot\nabla)^nf(x_0,y_0)$$
We can generalize this to 
the $k$-th order Taylor polynomial.
    $$P_kf(\mathbf x) = \sum_{n=0}^k\frac{1}{n!}
    (\Delta \mathbf x\cdot\nabla)^nf(\mathbf x_0)$$
This is a generalization of the Taylor polynomial 
for single variable function: since 
$(\Delta x\cdot \nabla)^nf(x_0) = \Delta x^n\nabla^n f(x_0)$,
	$$P_k(x) = \sum_{n=0}^k \frac{f^{(n)}(x_0)\Delta x^n}{n!}$$
\end{frame}

\begin{frame}
\begin{thm}[Taylor]
Let $f(x,y)$ is a function whose 
third partial derivatives $f_{xxx},f_{xyx},\cdots,f_{yyy}$ 
are all continuous on a rectangular region 
	$$D=\{(x,y)\,|\,|x-x_0|,|y-y_0|\le\epsilon\}$$
Then for each $(x,y)\in D$, 
there exists a constant $0\le c\le 1$ satisfying
	$$f(x,y)=Q(x,y)+R_2(x,y)$$
where 
    $$R_2(x,y) = \frac{1}{3!}(\Delta\mathbf x\cdot\nabla)^3
    f(\mathbf x_0 + c\Delta \mathbf x)$$
\end{thm}
\end{frame}

\begin{frame}
\begin{rem}
This theorem generalizes the Taylor theorem for 
single-variable function:
\begin{quote}
Let $f$ be a $\mathcal C^{k+1}$-function on 
an interval $I = (x_0-\epsilon,x_0+\epsilon)$.
Then for $x,c\in I$, there exists a constant $\xi$ 
between $x$ and $c$ such that 
	$$f(x) = P_k(x) + \frac{f^{(k+1)}(\xi)}{(k+1)!}(x-c)^{k+1}$$
\end{quote}
Here $x=x_0+\Delta x$ and $\xi = x_0+c\Delta x$ 
for some $0\le c\le 1$.
Note that the choice of $c$ \emph{depends} on 
the choice of $x,x_0$.
We can approximate \emph{any} function by $Q_2(x,y)$
if it has partial derivatives upto 3rd order.
If $(x,y)$ is sufficiently close to $(x_0,y_0)$,
the error $|R_n(x,y)|$ decreases to zero.
\end{rem}
\end{frame}

\begin{frame}
\begin{exmp}
Find $Q_2(x,y)$ at $(0,0)$ for 
\begin{itemize}
    \item $f(x,y) = xy - x^2 - 5y^2 + y - 1$
    \item $f(x,y) = \cos x\cos y$
\end{itemize}
and compare the graphs of $Q_2$ and $f$ near $(1,0)$ 
\end{exmp}
\end{frame}

\begin{frame}
\begin{defn}\label{defn-crit-loc}
Let $f(x,y)$ be a function $f(x,y)$ defined on a region $D$.
\begin{itemize}
    \item A point $(x_0,y_0)$ is said to be 
    \textbf{local maximal} (\textbf{minimal}, respectively) 
    at $\mathbf x_0=(x_0,y_0)$
	if there exists a (sufficiently small) $\epsilon>0$ such that
    for all $\mathbf x=(x,y)$ satisfying 
    $\Vert\mathbf x-\mathbf x_0\Vert<\epsilon$, we have
    $f(x_0,y_0)\ge f(x,y)$ ($f(x_0,y_0)\le f(x,y)$, respectively). 
    A local maximal / minimal is often called an \textbf{extremal}.
    \item A point $(x_0,y_0)$ is called a \textbf{critical point} 
    if it satisfies one of the following.
	\begin{enumerate}
		\item $f_x(x_0,y_0)=f_y(x_0,y_0)=0$;
		\item $f_x$ or $f_y$ does not exist at $(x_0,y_0)$;
		\item $f$ is discontinuous at $(x_0,y_0)$.
	\end{enumerate}
    A critical point which is \emph{not} an extremal 
    is called a \textbf{saddle point}.
\end{itemize}
\end{defn}
\end{frame}

\begin{frame}
\begin{exmp}
Find the critical points of
    $$f(x,y) = xy- x^2y-xy^2$$
and classify them. Also, find $Q_2(x,y)$ at each critical points
and compare their graphs.
\end{exmp}
\end{frame}

\begin{frame}
\begin{rem}\*
\begin{enumerate}
	\item Let $f(x,y)$ is differentiable at $(x_0,y_0)$.
    Suppose that $(x_0,y_0)$ is a critical point $f(x,y)$ 
    of the type 1 in the definition.
    Then the linear approximation $(x_0,y_0)$ 
    is the plane $z=f(x_0,y_0)$.
	\item Suppose that $(x_0,y_0)$ is a saddle point.
    Then there exists a curve 
    $c:(-\epsilon,\epsilon)\to\mathbf R^2$, $c(0)=(x_0,y_0)$ 
    such that composition $F(t) = (f\circ c)(t)$ is a
    inflection point.
\end{enumerate}
\end{rem}	
\end{frame}

\begin{frame}
\begin{defn}
Let $R\subset\mathbf R^2$ be a domain of $f(x,y)$ and 
$(x_0,y_0)\in R$.
Suppose that all second partial derivatives of $f(x,y)$ 
are continuous on a region $R$. Then
    $$\Delta_f = f_{xx}(x_0,y_0)f_{yy}(x_0,y_0)
     - f_{xy}(x_0,y_0)^2$$
is called the \textbf{discriminant} of $f$.
\end{defn}
\begin{exmp}
Graph the following function at $(0,0)$ and compare their 
discriminants.
    $$z = -x^2-y^2,\quad z = x^2+y^2,\quad z = x^2-y^2$$
\end{exmp}
\end{frame}

\begin{frame}
\begin{thm}[Hesse]
Let $(x_0,y_0)$ be a critical point of the type 1 of $f(x,y)$.
	\begin{itemize}
        \item If $\Delta>0$ and $f_{xx}(x_0,y_0)>0$, 
        then $(x_0,y_0)$ is a local minimum point.
        \item If $\Delta>0$ and $f_{xx}(x_0,y_0)<0$, 
        then $(x_0,y_0)$ is a local maximum point.
		\item If $\Delta<0$, then $f(x_0,y_0)$ is a saddle point.
        \item If $\Delta=0$, then we cannot determine 
        local extremity by this method.
	\end{itemize}
\end{thm}
\end{frame}

\begin{frame}
\begin{rem}
Let $\Delta x= x-x_0,\Delta y=y-y_0$. 
The degree-$2$ summands of $Q(x,y)$ can be written as
	$$\frac{1}{2}\begin{bmatrix}\Delta x & \Delta y\end{bmatrix}
    \underbrace{\begin{bmatrix} f_{xx}(x_0,y_0) & f_{xy}(x_0,y_0)\\
	f_{xy}(x_0,y_0) & f_{yy}(x_0,y_0) \end{bmatrix}}_{=A}
	\begin{bmatrix}\Delta x \\ \Delta y\end{bmatrix}$$
Note that $\Delta_f=\det A$.
Using linear transformation of $x,y$, we can coorrespond
the matrix $A$ to one of three matrices below, wihout
changing the classifications of extremals.
	$$\begin{bmatrix}-1&0\\0&-1\end{bmatrix},\quad
	\begin{bmatrix}1&0\\0&1\end{bmatrix},\quad
	\begin{bmatrix}\pm1&0\\0&\mp1\end{bmatrix}$$
\end{rem}
\end{frame}

\begin{frame}
\begin{exmp}[Least square method]
	Suppose that a set of data is given by
		$$(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)$$
	We want to find a line $y=mx+b$ which approximates these data.
    If there are values $m_0,b_0$ such that
    the sum of squares of vertical distances between 
    data and the line $y=m_0x+b_0$ is the minimum 
    among all possible lines, then 
	we say that $y=m_0x+b_0$ best approximates the data.
	In other words, we want to find $m,b$ such that
		$$d(m,b) = \sum_{i=1}^n(y_i-(mx_i+b))^2$$
    is minimum.
\end{exmp}
\end{frame}

\begin{frame}    
Consider $d(m,b)$ as two-variable function on $m,b$.
The critical point is
    $$m_0 = \frac{\displaystyle n\sum_{i=1}^nx_iy_i 
        - \sum_{i=1}^n x_i\sum_{i=1}^n y_i}
        {\displaystyle n\sum_{i=1}^n x_i^2 
        - \left(\sum_{i=1}^nx_i\right)^2}$$
    $$b_0 = \frac{\displaystyle \sum_{i=1}^nx_i^2\sum_{i=1}^ny_i
        - \sum_{i=1}^nx_i\sum_{i=1}^nx_iy_i}
        {\displaystyle n\sum_{i=1}^nx_i^2 
        - \left(\sum_{i=1}^nx_i\right)^2}$$
	The Hessian of $d(m,b)$ at $(m_0,b_0)$ is
	$\begin{bmatrix}
	\displaystyle 2\sum_{i=1}^n x_i^2 & \displaystyle 2\sum_{i=1}^n x_i \\
    \displaystyle 2\sum_{i=1}^n x_i & 2n \end{bmatrix}$
\end{frame}

\begin{frame}
Since the discriminant of $f$ is
    $$\Delta_f(m_0,b_0) = 4n\sum_{i=1}^nx_i^2-4
    \left(\sum_{i=1}^nx_i\right)^2 > 0$$
The point $(m_0,b_0)$ is a local minimum point.
(in fact a global minimum point, why?)
\end{frame}

\begin{frame}
\begin{thm}
Let $(x_0,y_0,z_0)$ be a critical point of $f(x,y,z)$
where $f_x,f_y,f_z$ are all zero.
Let $H$ be the $3\times 3$ matrix defined by
    $$\begin{bmatrix} f_{xx} & f_{xy} & f_{xz} \\
        f_{yx} & f_{yy} & f_{yz} \\
        f_{zx} & f_{zy} & f_{zz} \end{bmatrix}$$
Let $d_1, d_2, d_3$ be the determinants of the 
$1\times1$, $2\times2$, $3\times3$ sub-matrices on the 
left-top corner of $H$. 
    \begin{itemize}
       \item If $d_i > 0$ for all $i$, then $(x_0,y_0,z_0)$
       is a local minimum point. 
       \item if $d_1, d_3 < 0$ and $d_2 > 0$, then
       $(x_0,y_0,z_0)$ is a local maximal point.
       \item In all other cases, $(x_0,y_0,z_0)$ is a saddle point.
    \end{itemize}
\end{thm}
\end{frame}

\begin{frame}
\begin{prop}
    Let $L_c(f)$ be the level curve at $c = f(x_0,y_0)$.
    on the $xy$-plane. 
    Then the gradient vector $\nabla f(x_0,y_0)$ 
    is perpendicular to the curve $L_c(f)$ at $(x_0,y_0)$ 
\end{prop}
\end{frame}

\begin{frame}
\begin{thm}[Lagrange multiplier]
Let $g(x,y)$, $f(x,y)$ be differentiable functions.
Let $L_c(g)$ be a level curve at $c$.
Let us retrict the domain of $f$ onto $L_c(g)$.
If $(x_0,y_0)$ is an extremal point of $f$ 
and $\nabla g(x_0,y_0)\neq\mathbf 0$,
there exists $\lambda$ such that
	$$\nabla f(x_0,y_0) = \lambda\nabla g(x_0,y_0)$$
\end{thm}
\end{frame}

\begin{frame}
\begin{exmp}
The Lagrange multiplier finds the maxima or minima of a 
\textbf{target} function $f(x,y)$ under the
\textbf{constraint} $g(x,y) = c$.
Find the point on the circle $x^2+y^2 = 10$ 
where the function $f(x,y)=3x+y$ attains maximal or minimal.
\end{exmp}

\begin{cor}
The gradient vector $\nabla f(x_0,y_0)$ has the 
direction where the value of function $f(x,y)$ 
increases the most from $(x_0,y_0)$. 
\end{cor}
\end{frame}

\begin{frame}
\begin{exmp}
Let
	$$f(x,y)=100+\frac{100}{x^2+2y^2+9}$$
be a function whose graph represent the contour of a mountain.
Suppose that a water flow from the point $(1,0,110)$ 
down the valley in the steepest direction.
Find the trajectory of the water path.
\begin{center}
	\includegraphics[scale=.2]{image/lec4-1}
\end{center}
\end{exmp}
\end{frame}

\begin{frame}
\begin{thm}
Let $g(x,y,z)$, $f(x,y,z)$ be differentiable functions.
Suppose that $(x_0,y_0,z_0)$ is a local extremal of $f(x,y,z)$ 
restricted the level set $L_c(g)$. 
If $\nabla g(x_0,y_0)\neq\mathbf 0$, 
then there exists $\lambda$ such that
	$$\nabla f(x_0,y_0,z_0)=\lambda\nabla g(x_0,y_0,z_0)$$
\end{thm}

\begin{exmp}
Find the minimal and maximal value of $f(x,y,z)=x^3+y^3+z^3$
on the sphere $x^2+y^2+z^2=1$ on the first octant.
\end{exmp}
\end{frame}

\begin{frame}
\begin{prob}
Find all critical points and classify them
\begin{enumerate}
    \item $\displaystyle f(x,y) = xy + \frac{2}{x} + \frac{2}{y}$
    \item $e^y(x^2+y^2-z^2)$
\end{enumerate}
\end{prob}
\end{frame}

\begin{frame}
\begin{prob}
Find all local extremes of $f(x,y)$ with the give contraints.
\begin{enumerate}
    \item $f(x,y) = 2x+y^2+x^2, 2x^2+y^2 = 2$
    \item $f(x,y,z) = xy+yz, x^2+y^2 = 1, yz = 1$
\end{enumerate}
\end{prob}
\end{frame}

\begin{frame}
\begin{prob}
Find the local extremes of $f(x,y) = x^2+xy+y^2$
on the disk $D = \{(x,y)\mid x^2+y^2\le 1\}$.
\end{prob}
\end{frame}

\begin{frame}
\begin{prob}
Find the point on the graph $xy^2z^3=2$ 
which is the closest to the origin.
\end{prob}
\end{frame}
\end{document}